<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Model Evaluation and Tuning - Machine Learning with Python Course">
    <title>Model Evaluation and Tuning</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Machine Learning with Python</h1>
    </header>

    <main>
        <section id="model-evaluation">
            <h2>Model Evaluation and Tuning</h2>
            <p>Model evaluation and tuning are crucial steps in the machine learning workflow. After developing a model, it is essential to evaluate its performance, identify areas for improvement, and fine-tune its parameters to achieve the best possible results. This section covers various techniques for evaluating and tuning machine learning models, ensuring that they generalize well to new, unseen data.</p>

            <h3>1. Understanding Model Evaluation</h3>
            <p>Model evaluation involves assessing a model's performance to ensure it effectively solves the problem at hand. The evaluation process provides insights into the model's strengths and weaknesses and helps guide further improvements. Several metrics and techniques are used to evaluate models, depending on the problem type (classification, regression, clustering, etc.).</p>
            <p>Some common evaluation techniques include:</p>
            <ul>
                <li><strong>Training and Test Split:</strong> Dividing the dataset into training and test sets to evaluate the model on unseen data.</li>
                <li><strong>Cross-Validation:</strong> Splitting the data into multiple folds and training the model on different combinations of these folds. This provides a more robust estimate of model performance.</li>
                <li><strong>Confusion Matrix:</strong> Evaluates the performance of a classification model by showing the count of true positives, false positives, true negatives, and false negatives.</li>
                <li><strong>ROC-AUC Curve:</strong> Used for evaluating binary classification models by plotting the true positive rate against the false positive rate at various threshold levels.</li>
            </ul>

            <h3>2. Evaluation Metrics for Classification Models</h3>
            <p>When evaluating classification models, it is important to use metrics that reflect how well the model can predict the classes of the target variable. Some of the commonly used metrics are:</p>

            <h4>2.1. Accuracy</h4>
            <p>Accuracy is the ratio of correctly predicted instances to the total number of instances. While it is a good indicator when the classes are balanced, it can be misleading when there is class imbalance.</p>
            <pre><code>from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}') </code></pre>

            <h4>2.2. Precision, Recall, and F1-Score</h4>
            <p>These metrics are particularly useful when dealing with imbalanced datasets:</p>
            <ul>
                <li><strong>Precision:</strong> The ratio of correctly predicted positive observations to the total predicted positives. High precision indicates a low false positive rate.</li>
                <li><strong>Recall:</strong> The ratio of correctly predicted positive observations to all actual positives. High recall indicates a low false negative rate.</li>
                <li><strong>F1-Score:</strong> The harmonic mean of precision and recall. It is used as a balanced metric when precision and recall are both important.</li>
            </ul>
            <pre><code>from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
print(f'Precision: {precision}, Recall: {recall}, F1-Score: {f1}') </code></pre>

            <h4>2.3. ROC-AUC Score</h4>
            <p>The Receiver Operating Characteristic (ROC) curve plots the true positive rate (sensitivity) against the false positive rate (1 - specificity). The Area Under the ROC Curve (AUC) represents the model’s ability to distinguish between classes.</p>
            <pre><code>from sklearn.metrics import roc_auc_score

roc_auc = roc_auc_score(y_test, y_pred)
print(f'ROC-AUC Score: {roc_auc}') </code></pre>

            <h3>3. Evaluation Metrics for Regression Models</h3>
            <p>For regression models, evaluation metrics focus on the differences between the predicted and actual values. Some commonly used metrics are:</p>

            <h4>3.1. Mean Absolute Error (MAE)</h4>
            <p>MAE is the average of the absolute differences between the predicted and actual values. It is a straightforward metric that represents the error magnitude.</p>
            <pre><code>from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_test, y_pred)
print(f'Mean Absolute Error: {mae}') </code></pre>

            <h4>3.2. Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)</h4>
            <p>MSE is the average of the squared differences between the predicted and actual values. RMSE is the square root of MSE and provides a metric in the same units as the target variable.</p>
            <pre><code>from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_test, y_pred)
rmse = mse**0.5
print(f'Mean Squared Error: {mse}, Root Mean Squared Error: {rmse}') </code></pre>

            <h4>3.3. R-Squared (R²)</h4>
            <p>R² represents the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, with higher values indicating a better fit.</p>
            <pre><code>from sklearn.metrics import r2_score

r2 = r2_score(y_test, y_pred)
print(f'R-Squared: {r2}') </code></pre>

            <h3>4. Hyperparameter Tuning</h3>
            <p>Hyperparameters are parameters that control the model’s learning process and cannot be learned from the data. They are set before training and play a crucial role in model performance. Hyperparameter tuning involves finding the optimal set of hyperparameters for a given model.</p>
            <p>Common techniques for hyperparameter tuning include:</p>
            <ul>
                <li><strong>Grid Search:</strong> Evaluates a model’s performance over a specified grid of hyperparameter values.</li>
                <li><strong>Random Search:</strong> Randomly samples hyperparameter combinations from a defined range and evaluates performance.</li>
                <li><strong>Bayesian Optimization:</strong> Uses probabilistic models to find the optimal hyperparameters based on prior evaluations.</li>
            </ul>

            <h4>4.1. Grid Search</h4>
            <p>Grid search exhaustively evaluates a model’s performance over all combinations of hyperparameter values provided in a grid. It is useful when the hyperparameter space is small, but can be computationally expensive for larger spaces.</p>
            <pre><code>from sklearn.model_selection import GridSearchCV

# Define the hyperparameter grid
param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)
grid.fit(X_train, y_train)

print(f'Best Parameters: {grid.best_params_}') </code></pre>

            <h4>4.2. Random Search</h4>
            <p>Random search randomly samples hyperparameters and evaluates performance. It is less computationally expensive than grid search and can find optimal parameters more efficiently.</p>
            <pre><code>from sklearn.model_selection import RandomizedSearchCV

# Define the parameter distributions
param_dist = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=5)
random_search.fit(X_train, y_train)

print(f'Best Parameters: {random_search.best_params_}') </code></pre>

            <h3>5. Final Thoughts on Model Evaluation and Tuning</h3>
            <p>Model evaluation and tuning are critical steps in the machine learning process. Proper evaluation helps ensure that the model generalizes well to new data, while tuning optimizes its performance. By choosing appropriate evaluation metrics and tuning methods, you can develop robust models that perform well on a variety of real-world problems.</p>
        </section>
    </main>

    <footer>
        <p>&copy; 2024 E-Learning Platform. All rights reserved.</p>
    </footer>
</body>
</html>
